mantel(envata.abund,dist.abund,method = "spearman", permutations = 9999, na.rm = TRUE)
dbpata<-read.xlsx("C:/Users/USER/Desktop/lab/實驗/Metagenomic in DWDS/DATA/newDATA/ARG/ARGoap_out.xlsx",sheet=1,rowNames=T,colNames=T,sep.names=" ")
groupata<-read.xlsx("C:/Users/USER/Desktop/lab/實驗/Metagenomic in DWDS/DATA/newDATA/TAXA/groupdata.xlsx",sheet=1,rowNames=T,colNames=T,sep.names=" ")
envata<-read.xlsx("C:/Users/USER/Desktop/lab/實驗/Metagenomic in DWDS/DATA/newDATA/TAXA/rel abundance table/genus_rel_table.xlsx",sheet=1,rowNames=F,colNames=T,sep.names=" ")
rownames(envata)<-envata$Genus
envata<-envata[,-(1:7)]
dbpata<-dbpata[apply(dbpata, 1, function(x) !all(x==0)),]
dbpata<-as.data.frame(t(dbpata))
dbpata<-read.xlsx("C:/Users/USER/Desktop/lab/實驗/Metagenomic in DWDS/DATA/newDATA/ARG/ARGoap_out.xlsx",sheet=1,rowNames=T,colNames=T,sep.names=" ")
groupata<-read.xlsx("C:/Users/USER/Desktop/lab/實驗/Metagenomic in DWDS/DATA/newDATA/TAXA/groupdata.xlsx",sheet=1,rowNames=T,colNames=T,sep.names=" ")
envata<-read.xlsx("C:/Users/USER/Desktop/lab/實驗/Metagenomic in DWDS/DATA/newDATA/TAXA/rel abundance table/genus_rel_table.xlsx",sheet=1,rowNames=F,colNames=T,sep.names=" ")
rownames(envata)<-envata$Genus
envata<-envata[,-(1:7)]
dbpata<-dbpata[apply(dbpata, 1, function(x) !all(x==0)),]
dbpata<-as.data.frame(t(dbpata))
envata <- decostand(envata, method = 'hellinger')
dbpata<-dbpata[-(1:3),]
envata<-envata[-(1:3),]
dbpata<-read.xlsx("C:/Users/USER/Desktop/lab/實驗/Metagenomic in DWDS/DATA/newDATA/ARG/ARGoap_out.xlsx",sheet=1,rowNames=T,colNames=T,sep.names=" ")
groupata<-read.xlsx("C:/Users/USER/Desktop/lab/實驗/Metagenomic in DWDS/DATA/newDATA/TAXA/groupdata.xlsx",sheet=1,rowNames=T,colNames=T,sep.names=" ")
envata<-read.xlsx("C:/Users/USER/Desktop/lab/實驗/Metagenomic in DWDS/DATA/newDATA/TAXA/rel abundance table/genus_rel_table.xlsx",sheet=1,rowNames=F,colNames=T,sep.names=" ")
rownames(envata)<-envata$Genus
envata<-envata[,-(1:7)]
dbpata<-dbpata[apply(dbpata, 1, function(x) !all(x==0)),]
dbpata<-as.data.frame(t(dbpata))
envata <- decostand(envata, method = 'hellinger')
dbpata<-dbpata[-(1:3),]
envata<-envata[,-(1:3)]
groupata<-groupata[-(1:3),]
envata$sum<-apply(envata,1,sum)
envata<-envata%>%
arrange(desc(sum))
envata$sum<-NULL
envata<-envata[1:30,]
envata<-as.data.frame(t(envata))
#dbpata<-dbpata[-c(2,6,11),]
#envata<-envata[-c(2,6,11),]
dist.abund <- vegdist(dbpata, method = "bray")
envata.abund <- vegdist(envata, method = "bray")
dist.abund<-as.matrix(dist.abund)
envata.abund <-as.matrix(envata.abund)
each_column_dist<-envata%>%
summarise_all(~vegdist(.,method = "bray"))%>%
as.data.frame()
View(envata)
mantel(envata.abund,dist.abund,method = "spearman", permutations = 9999, na.rm = TRUE)
mantel(envata.abund,dist.abund,method = "pearson", permutations = 9999, na.rm = TRUE)
spearman
?summarise_all
each_column_dist<-envata%>%
summarise_all(~vegdist(.,method = "bray"))
View(each_column_dist)
envata1<-envata
envata1$g__Polycladomycesvegdist(envata$g__Polycladomyces, method = "bray")
envata1$g__Polycladomyces<-vegdist(envata$g__Polycladomyces, method = "bray")
envata1<-vegdist(envata$g__Polycladomyces, method = "bray")
as.matrix(envata1)
View(dist.abund)
each_column_dist<-envata%>%
summarise_all(~vegdist(.,method = "bray"))%>%
as.list()
View(each_column_dist)
for(i in 1:n){
x<-mantel(each_column_dist[i],dist.abund, method = "spearman", permutations = 9999, na.rm = TRUE)
df[1,i]<-x$statistic
df[2,i]<-x$signif
}
View(each_column_dist)
View(dbpata)
View(envata)
each_column_dist<-envata%>%
summarise_all(~vegdist(.,method = "bray"))
View(each_column_dist)
View(dist.abund)
View(envata)
each_column_dist<-envata%>%
summarise_all(~vegdist(.,method = "bray"))%>%
as.data.frame()
#dbpata<-dbpata[-c(2,6,11),]
#envata<-envata[-c(2,6,11),]
dist.abund <- vegdist(dbpata, method = "bray")
mantel.pairwise<-function(){
df<- data.frame()
n<-length(colnames(envata))
for(i in 1:n){
x<-mantel(each_column_dist[,i],dist.abund, method = "spearman", permutations = 9999, na.rm = TRUE)
df[1,i]<-x$statistic
df[2,i]<-x$signif
}
colnames(df)<-colnames(each_column_dist)
rownames(df)<-c("Mantel statistic r","Significance")
assign("mantel_output",df,envir = globalenv())
}
mantel.pairwise()
View(mantel_output)
View(mantel_output)
library("PMCMRplus")
library("dplyr")
library("tidyverse")
library("openxlsx")
library("stringr")
library("car")
library("FSA")
library("RColorBrewer")
library("ggsignif")
library("mdthemes")
data<-read.xlsx(xlsxFile = "C:/Users/USER/Desktop/lab/實驗/Metagenomic in DWDS/DATA/newDATA/ARG/argoap_out.xlsx",sheet=2,rowNames=T,colNames=T)
#目的:比對某參數在兩個季節採樣間是否有顯著差異(排除原水)
data<-data[apply(data, 1, function(x) !all(x==0)),]
data<-as.data.frame(t(data))
data$sum<-apply(data,1,sum)
data$location<-c("Raw","Raw","Raw","Finished","Finished","Finished","Upstream","Upstream","Upstream","Midstream","Midstream","Midstream","Downstream","Downstream","Downstream")
data$location<-factor(data$location,levels = c("Raw","Finished","Upstream","Midstream","Downstream"))
View(data)
varaible_and_group<-bacitracin~location#想測試的變數跟組別
#我們必須先檢查數據是不是常態分布及變異數的同質性，才能決定我們要用的檢定方法。
#檢查數據變異數的同質性，可以使用levenes test
#如果levenes test 的結果p>0.05，那我們可以認為以這幾個組別間的變異數沒有明顯差異，他們是同質的P
{homo<-leveneTest(varaible_and_group,data = data)
homo
if (homo$`Pr(>F)`[1]>0.05){
print("data is homo")
}else{print ("data is nonhomo")}
#接著檢查數據是否是常態分布的
res.aov <- aov(varaible_and_group, data = data)
plot(res.aov,2)#這個是QQplot 可以透過這張圖來看一下有哪些點可以拿掉。
aov_residuals <- residuals(object = res.aov )
#利用shapiro.test來檢驗數據是不是常態的，如果p>0.05那麼數據就是常態的
a<-shapiro.test(x = aov_residuals)
if(a["p.value"]>0.05){
print("data is normal distribution")
}else{print("data is  not a normal distribution")}
#如果沒問題(常態且同質)就可以看ANOVA的結果了
if (a["p.value"]>0.05 && homo$`Pr(>F)`[1]>0.05){
#如果p<0.05，那表是多組間的差距是不同的，那我們可以用事後檢定來看到底是誰不一樣
print ("we can use anova ")
anova_p<-summary(res.aov)
if (anova_p[[1]]$`Pr(>F)`[1]<0.05){
print ("anova p<0.05, use TukeyHSD")
TukeyHSD(res.aov)
}else{
print("anova p>0.05,difference is insignificiant")
}
}else{
#數據在anova的兩項假設中有一項不符合，因此我們要使用kruskal-wallis來檢定
print ("use kruskal wallise rank sum test")
kruskal_output<-kruskal.test(varaible_and_group, data = data)
if (kruskal_output$p.value< 0.05){
#kruskal-wallis 檢定 p <0.05，表示組間有差距，因此我們要使用事後檢定
#我們可以使用Dunntest來看看是哪一組不同
PT = dunnTest(varaible_and_group, data = data,
method="bh")    # Can adjust p-values;
print (PT)
}else{
print ("kruskal-wallis p>0.05,difference is insignificiant")
}
}
}
varaible_and_group<-multidrug~location#想測試的變數跟組別
#我們必須先檢查數據是不是常態分布及變異數的同質性，才能決定我們要用的檢定方法。
#檢查數據變異數的同質性，可以使用levenes test
#如果levenes test 的結果p>0.05，那我們可以認為以這幾個組別間的變異數沒有明顯差異，他們是同質的P
{homo<-leveneTest(varaible_and_group,data = data)
homo
if (homo$`Pr(>F)`[1]>0.05){
print("data is homo")
}else{print ("data is nonhomo")}
#接著檢查數據是否是常態分布的
res.aov <- aov(varaible_and_group, data = data)
plot(res.aov,2)#這個是QQplot 可以透過這張圖來看一下有哪些點可以拿掉。
aov_residuals <- residuals(object = res.aov )
#利用shapiro.test來檢驗數據是不是常態的，如果p>0.05那麼數據就是常態的
a<-shapiro.test(x = aov_residuals)
if(a["p.value"]>0.05){
print("data is normal distribution")
}else{print("data is  not a normal distribution")}
#如果沒問題(常態且同質)就可以看ANOVA的結果了
if (a["p.value"]>0.05 && homo$`Pr(>F)`[1]>0.05){
#如果p<0.05，那表是多組間的差距是不同的，那我們可以用事後檢定來看到底是誰不一樣
print ("we can use anova ")
anova_p<-summary(res.aov)
if (anova_p[[1]]$`Pr(>F)`[1]<0.05){
print ("anova p<0.05, use TukeyHSD")
TukeyHSD(res.aov)
}else{
print("anova p>0.05,difference is insignificiant")
}
}else{
#數據在anova的兩項假設中有一項不符合，因此我們要使用kruskal-wallis來檢定
print ("use kruskal wallise rank sum test")
kruskal_output<-kruskal.test(varaible_and_group, data = data)
if (kruskal_output$p.value< 0.05){
#kruskal-wallis 檢定 p <0.05，表示組間有差距，因此我們要使用事後檢定
#我們可以使用Dunntest來看看是哪一組不同
PT = dunnTest(varaible_and_group, data = data,
method="bh")    # Can adjust p-values;
print (PT)
}else{
print ("kruskal-wallis p>0.05,difference is insignificiant")
}
}
}
varaible_and_group<-tetracycline~location#想測試的變數跟組別
#我們必須先檢查數據是不是常態分布及變異數的同質性，才能決定我們要用的檢定方法。
#檢查數據變異數的同質性，可以使用levenes test
#如果levenes test 的結果p>0.05，那我們可以認為以這幾個組別間的變異數沒有明顯差異，他們是同質的P
{homo<-leveneTest(varaible_and_group,data = data)
homo
if (homo$`Pr(>F)`[1]>0.05){
print("data is homo")
}else{print ("data is nonhomo")}
#接著檢查數據是否是常態分布的
res.aov <- aov(varaible_and_group, data = data)
plot(res.aov,2)#這個是QQplot 可以透過這張圖來看一下有哪些點可以拿掉。
aov_residuals <- residuals(object = res.aov )
#利用shapiro.test來檢驗數據是不是常態的，如果p>0.05那麼數據就是常態的
a<-shapiro.test(x = aov_residuals)
if(a["p.value"]>0.05){
print("data is normal distribution")
}else{print("data is  not a normal distribution")}
#如果沒問題(常態且同質)就可以看ANOVA的結果了
if (a["p.value"]>0.05 && homo$`Pr(>F)`[1]>0.05){
#如果p<0.05，那表是多組間的差距是不同的，那我們可以用事後檢定來看到底是誰不一樣
print ("we can use anova ")
anova_p<-summary(res.aov)
if (anova_p[[1]]$`Pr(>F)`[1]<0.05){
print ("anova p<0.05, use TukeyHSD")
TukeyHSD(res.aov)
}else{
print("anova p>0.05,difference is insignificiant")
}
}else{
#數據在anova的兩項假設中有一項不符合，因此我們要使用kruskal-wallis來檢定
print ("use kruskal wallise rank sum test")
kruskal_output<-kruskal.test(varaible_and_group, data = data)
if (kruskal_output$p.value< 0.05){
#kruskal-wallis 檢定 p <0.05，表示組間有差距，因此我們要使用事後檢定
#我們可以使用Dunntest來看看是哪一組不同
PT = dunnTest(varaible_and_group, data = data,
method="bh")    # Can adjust p-values;
print (PT)
}else{
print ("kruskal-wallis p>0.05,difference is insignificiant")
}
}
}
#以下開始繪圖，先計算平均跟標準差，其實這部不是必要
varaible_group_mean<-data%>%
group_by(type)%>%
summarise(type_mean=mean(Temperature,na.rm=T),type_sd=sd(Temperature))
varaible_and_group<-tetracyclin~location#想測試的變數跟組別
#我們必須先檢查數據是不是常態分布及變異數的同質性，才能決定我們要用的檢定方法。
#檢查數據變異數的同質性，可以使用levenes test
#如果levenes test 的結果p>0.05，那我們可以認為以這幾個組別間的變異數沒有明顯差異，他們是同質的P
{homo<-leveneTest(varaible_and_group,data = data)
homo
if (homo$`Pr(>F)`[1]>0.05){
print("data is homo")
}else{print ("data is nonhomo")}
#接著檢查數據是否是常態分布的
res.aov <- aov(varaible_and_group, data = data)
plot(res.aov,2)#這個是QQplot 可以透過這張圖來看一下有哪些點可以拿掉。
aov_residuals <- residuals(object = res.aov )
#利用shapiro.test來檢驗數據是不是常態的，如果p>0.05那麼數據就是常態的
a<-shapiro.test(x = aov_residuals)
if(a["p.value"]>0.05){
print("data is normal distribution")
}else{print("data is  not a normal distribution")}
#如果沒問題(常態且同質)就可以看ANOVA的結果了
if (a["p.value"]>0.05 && homo$`Pr(>F)`[1]>0.05){
#如果p<0.05，那表是多組間的差距是不同的，那我們可以用事後檢定來看到底是誰不一樣
print ("we can use anova ")
anova_p<-summary(res.aov)
if (anova_p[[1]]$`Pr(>F)`[1]<0.05){
print ("anova p<0.05, use TukeyHSD")
TukeyHSD(res.aov)
}else{
print("anova p>0.05,difference is insignificiant")
}
}else{
#數據在anova的兩項假設中有一項不符合，因此我們要使用kruskal-wallis來檢定
print ("use kruskal wallise rank sum test")
kruskal_output<-kruskal.test(varaible_and_group, data = data)
if (kruskal_output$p.value< 0.05){
#kruskal-wallis 檢定 p <0.05，表示組間有差距，因此我們要使用事後檢定
#我們可以使用Dunntest來看看是哪一組不同
PT = dunnTest(varaible_and_group, data = data,
method="bh")    # Can adjust p-values;
print (PT)
}else{
print ("kruskal-wallis p>0.05,difference is insignificiant")
}
}
}
varaible_and_group<-tetracycline~location#想測試的變數跟組別
#我們必須先檢查數據是不是常態分布及變異數的同質性，才能決定我們要用的檢定方法。
#檢查數據變異數的同質性，可以使用levenes test
#如果levenes test 的結果p>0.05，那我們可以認為以這幾個組別間的變異數沒有明顯差異，他們是同質的P
{homo<-leveneTest(varaible_and_group,data = data)
homo
if (homo$`Pr(>F)`[1]>0.05){
print("data is homo")
}else{print ("data is nonhomo")}
#接著檢查數據是否是常態分布的
res.aov <- aov(varaible_and_group, data = data)
plot(res.aov,2)#這個是QQplot 可以透過這張圖來看一下有哪些點可以拿掉。
aov_residuals <- residuals(object = res.aov )
#利用shapiro.test來檢驗數據是不是常態的，如果p>0.05那麼數據就是常態的
a<-shapiro.test(x = aov_residuals)
if(a["p.value"]>0.05){
print("data is normal distribution")
}else{print("data is  not a normal distribution")}
#如果沒問題(常態且同質)就可以看ANOVA的結果了
if (a["p.value"]>0.05 && homo$`Pr(>F)`[1]>0.05){
#如果p<0.05，那表是多組間的差距是不同的，那我們可以用事後檢定來看到底是誰不一樣
print ("we can use anova ")
anova_p<-summary(res.aov)
if (anova_p[[1]]$`Pr(>F)`[1]<0.05){
print ("anova p<0.05, use TukeyHSD")
TukeyHSD(res.aov)
}else{
print("anova p>0.05,difference is insignificiant")
}
}else{
#數據在anova的兩項假設中有一項不符合，因此我們要使用kruskal-wallis來檢定
print ("use kruskal wallise rank sum test")
kruskal_output<-kruskal.test(varaible_and_group, data = data)
if (kruskal_output$p.value< 0.05){
#kruskal-wallis 檢定 p <0.05，表示組間有差距，因此我們要使用事後檢定
#我們可以使用Dunntest來看看是哪一組不同
PT = dunnTest(varaible_and_group, data = data,
method="bh")    # Can adjust p-values;
print (PT)
}else{
print ("kruskal-wallis p>0.05,difference is insignificiant")
}
}
}
View(data)
View(data)
colnames(data)[colnames(data)=="macrolide-lincosamide-streptogramin"]<-"MLS"
colnames(data)[colnames(data)=="beta-lactam"]<-"BL"
varaible_and_group<-BL~location#想測試的變數跟組別
#我們必須先檢查數據是不是常態分布及變異數的同質性，才能決定我們要用的檢定方法。
#檢查數據變異數的同質性，可以使用levenes test
#如果levenes test 的結果p>0.05，那我們可以認為以這幾個組別間的變異數沒有明顯差異，他們是同質的P
{homo<-leveneTest(varaible_and_group,data = data)
homo
if (homo$`Pr(>F)`[1]>0.05){
print("data is homo")
}else{print ("data is nonhomo")}
#接著檢查數據是否是常態分布的
res.aov <- aov(varaible_and_group, data = data)
plot(res.aov,2)#這個是QQplot 可以透過這張圖來看一下有哪些點可以拿掉。
aov_residuals <- residuals(object = res.aov )
#利用shapiro.test來檢驗數據是不是常態的，如果p>0.05那麼數據就是常態的
a<-shapiro.test(x = aov_residuals)
if(a["p.value"]>0.05){
print("data is normal distribution")
}else{print("data is  not a normal distribution")}
#如果沒問題(常態且同質)就可以看ANOVA的結果了
if (a["p.value"]>0.05 && homo$`Pr(>F)`[1]>0.05){
#如果p<0.05，那表是多組間的差距是不同的，那我們可以用事後檢定來看到底是誰不一樣
print ("we can use anova ")
anova_p<-summary(res.aov)
if (anova_p[[1]]$`Pr(>F)`[1]<0.05){
print ("anova p<0.05, use TukeyHSD")
TukeyHSD(res.aov)
}else{
print("anova p>0.05,difference is insignificiant")
}
}else{
#數據在anova的兩項假設中有一項不符合，因此我們要使用kruskal-wallis來檢定
print ("use kruskal wallise rank sum test")
kruskal_output<-kruskal.test(varaible_and_group, data = data)
if (kruskal_output$p.value< 0.05){
#kruskal-wallis 檢定 p <0.05，表示組間有差距，因此我們要使用事後檢定
#我們可以使用Dunntest來看看是哪一組不同
PT = dunnTest(varaible_and_group, data = data,
method="bh")    # Can adjust p-values;
print (PT)
}else{
print ("kruskal-wallis p>0.05,difference is insignificiant")
}
}
}
varaible_and_group<-unclassified~location#想測試的變數跟組別
#我們必須先檢查數據是不是常態分布及變異數的同質性，才能決定我們要用的檢定方法。
#檢查數據變異數的同質性，可以使用levenes test
#如果levenes test 的結果p>0.05，那我們可以認為以這幾個組別間的變異數沒有明顯差異，他們是同質的P
{homo<-leveneTest(varaible_and_group,data = data)
homo
if (homo$`Pr(>F)`[1]>0.05){
print("data is homo")
}else{print ("data is nonhomo")}
#接著檢查數據是否是常態分布的
res.aov <- aov(varaible_and_group, data = data)
plot(res.aov,2)#這個是QQplot 可以透過這張圖來看一下有哪些點可以拿掉。
aov_residuals <- residuals(object = res.aov )
#利用shapiro.test來檢驗數據是不是常態的，如果p>0.05那麼數據就是常態的
a<-shapiro.test(x = aov_residuals)
if(a["p.value"]>0.05){
print("data is normal distribution")
}else{print("data is  not a normal distribution")}
#如果沒問題(常態且同質)就可以看ANOVA的結果了
if (a["p.value"]>0.05 && homo$`Pr(>F)`[1]>0.05){
#如果p<0.05，那表是多組間的差距是不同的，那我們可以用事後檢定來看到底是誰不一樣
print ("we can use anova ")
anova_p<-summary(res.aov)
if (anova_p[[1]]$`Pr(>F)`[1]<0.05){
print ("anova p<0.05, use TukeyHSD")
TukeyHSD(res.aov)
}else{
print("anova p>0.05,difference is insignificiant")
}
}else{
#數據在anova的兩項假設中有一項不符合，因此我們要使用kruskal-wallis來檢定
print ("use kruskal wallise rank sum test")
kruskal_output<-kruskal.test(varaible_and_group, data = data)
if (kruskal_output$p.value< 0.05){
#kruskal-wallis 檢定 p <0.05，表示組間有差距，因此我們要使用事後檢定
#我們可以使用Dunntest來看看是哪一組不同
PT = dunnTest(varaible_and_group, data = data,
method="bh")    # Can adjust p-values;
print (PT)
}else{
print ("kruskal-wallis p>0.05,difference is insignificiant")
}
}
}
#以下開始繪圖，先計算平均跟標準差，其實這部不是必要
varaible_group_mean<-data%>%
group_by(type)%>%
summarise(type_mean=mean(Temperature,na.rm=T),type_sd=sd(Temperature))
library("openxlsx")
library("Hmisc")
library("tidyverse")
data_ARGsub<-read.xlsx("C:/Users/USER/Desktop/lab/實驗/Metagenomic in DWDS/DATA/newDATA/ARG/ARGoap_out.xlsx",sheet=1,rowNames=F,sep.names=" ")
data_taxa<-read.xlsx("C:/Users/USER/Desktop/lab/實驗/Metagenomic in DWDS/DATA/newDATA/TAXA/rel abundance table/genus_rel_table.xlsx",sheet=1,rowNames=F,sep.names=" ")
#ARGdata名稱處理
data_ARGsub<-data_ARGsub%>%
separate(`ARGs abundance normalization aganist 16S`,into=c("type","subtype"),sep="__")
rownames(data_ARGsub)<-data_ARGsub$subtype
argclass<-data_ARGsub[,1:2]
data_ARGsub$type<-NULL
data_ARGsub$subtype<-NULL
data_ARGsub<-data_ARGsub[apply(data_ARGsub, 1, function(x) !all(x==0)),]
#此部分先篩選出在所有樣品中出現超過幾次之species，input型態row="species',col='sample'
data_clean<-data
data_clean[data_clean!=0]<-1
data_clean$times_discover_in_all<-apply(data_clean,1,sum)
data$times_discover_in_all<-data_clean$times_discover_in_all
library("openxlsx")
library("Hmisc")
library("tidyverse")
data_ARGsub<-read.xlsx("C:/Users/USER/Desktop/lab/實驗/Metagenomic in DWDS/DATA/newDATA/ARG/ARGoap_out.xlsx",sheet=1,rowNames=F,sep.names=" ")
data_taxa<-read.xlsx("C:/Users/USER/Desktop/lab/實驗/Metagenomic in DWDS/DATA/newDATA/TAXA/rel abundance table/genus_rel_table.xlsx",sheet=1,rowNames=F,sep.names=" ")
#ARGdata名稱處理
data_ARGsub<-data_ARGsub%>%
separate(`ARGs abundance normalization aganist 16S`,into=c("type","subtype"),sep="__")
rownames(data_ARGsub)<-data_ARGsub$subtype
argclass<-data_ARGsub[,1:2]
data_ARGsub$type<-NULL
data_ARGsub$subtype<-NULL
data_ARGsub<-data_ARGsub[apply(data_ARGsub, 1, function(x) !all(x==0)),]
#此部分先篩選出在所有樣品中出現超過幾次之species，input型態row="species',col='sample'
data_clean<-data_ARGsub
data_clean[data_clean!=0]<-1
data_clean$times_discover_in_all<-apply(data_clean,1,sum)
data$times_discover_in_all<-data_clean$times_discover_in_all
View(data_ARGsub)
#此部分先篩選出在所有樣品中出現超過幾次之species，input型態row="species',col='sample'
data_clean<-data_ARGsub
data_clean[data_clean!=0]<-1
data_clean$times_discover_in_all<-apply(data_clean,1,sum)
dataARGsub$times_discover_in_all<-data_clean$times_discover_in_all
data_ARGsub$times_discover_in_all<-data_clean$times_discover_in_all
times_over7_genusdata<-filter(data,times_discover_in_all>=7)
library("tidyverse")
times_over7_genusdata<-filter(data_ARGsub,times_discover_in_all>=7)
times_over7_genusdata$times_discover_in_all<-NULL
transform_data<-as.data.frame(t(times_over7_genusdata))
View(times_over7_genusdata)
View(times_over7_genusdata)
transform_data<-as.data.frame(t(times_over7_genusdata))
#以下部分開始進行相關性分析
data<-transform_data
#rcorr()他的input要是matrix
data.matrix<-as.matrix(data)
corr<-rcorr(data.matrix,type= 'spearman')
#P值修正
corr_P_adj <- p.adjust(corr$P, method = 'BH')
matrix_corr_P_adj <- matrix(corr_P_adj,nrow=(length(corr$P)**0.5))
colnames(matrix_corr_P_adj)<-colnames(data)
rownames(matrix_corr_P_adj) <- colnames(data)
matrix_corr_P_adj[matrix_corr_P_adj>= 0.05] <- -1
matrix_corr_P_adj[matrix_corr_P_adj < 0.05 & matrix_corr_P_adj >= 0] <- 1
matrix_corr_P_adj[matrix_corr_P_adj == -1] <- 0
#先輸出一次有顯著相關但相關性未必足夠的矩陣
corr_significiant<-corr$r * matrix_corr_P_adj
#write.xlsx之output要是dataframe
corr_significiant.dataframe<-as.data.frame(corr_significiant)
write.xlsx(corr_significiant.dataframe,file="c:/Users/USER/Desktop/corr_significiant_FAMILIES.xlsx",rowNames=T,colNames=T)
#將計算出來之相關性大於0.8且p小於0.05者留下
corr$r[corr$r < 0.8] <- 0
corr_final <-corr$r * matrix_corr_P_adj
#計算相關性只會有半邊的矩陣(上面是多餘的)，我們只會需要下三角矩陣，且不需要對角矩陣(都為1)
corr_final[!lower.tri(corr_final)] <- 0
#有些數據因為是0所以算不出相關性(na)，去除
corr_final[is.na(corr_final)]<-0
corr_final.dataframe<-as.data.frame(corr_final)
write.xlsx(corr_final.dataframe, 'C:/Users/USER/Desktop/FAMILES_R0.7P0.05.xlsx',rowNames=T,colNames=T)
