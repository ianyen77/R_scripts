}
}else{
#數據在anova的兩項假設中有一項不符合，因此我們要使用kruskal-wallis來檢定
print ("use kruskal wallise rank sum test")
kruskal_output<-kruskal.test(varaible_and_group, data = data)
if (kruskal_output$p.value< 0.05){
#kruskal-wallis 檢定 p <0.05，表示組間有差距，因此我們要使用事後檢定
#我們可以使用Dunntest來看看是哪一組不同
PT = dunnTest(varaible_and_group, data = data,
method="bh")    # Can adjust p-values;
print (PT)
}else{
print ("kruskal-wallis p>0.05,difference is insignificiant")
}
}
}
#以下開始繪圖，先計算平均跟標準差，其實這部不是必要
varaible_group_mean<-data%>%
group_by(type)%>%
summarise(type_mean=mean(Temperature,na.rm=T),type_sd=sd(Temperature))
library("openxlsx")
library("Hmisc")
library("tidyverse")
data_ARGsub<-read.xlsx("C:/Users/USER/Desktop/lab/實驗/Metagenomic in DWDS/DATA/newDATA/ARG/ARGoap_out.xlsx",sheet=1,rowNames=F,sep.names=" ")
data_taxa<-read.xlsx("C:/Users/USER/Desktop/lab/實驗/Metagenomic in DWDS/DATA/newDATA/TAXA/rel abundance table/genus_rel_table.xlsx",sheet=1,rowNames=F,sep.names=" ")
#ARGdata名稱處理
data_ARGsub<-data_ARGsub%>%
separate(`ARGs abundance normalization aganist 16S`,into=c("type","subtype"),sep="__")
rownames(data_ARGsub)<-data_ARGsub$subtype
argclass<-data_ARGsub[,1:2]
data_ARGsub$type<-NULL
data_ARGsub$subtype<-NULL
data_ARGsub<-data_ARGsub[apply(data_ARGsub, 1, function(x) !all(x==0)),]
#此部分先篩選出在所有樣品中出現超過幾次之species，input型態row="species',col='sample'
data_clean<-data
data_clean[data_clean!=0]<-1
data_clean$times_discover_in_all<-apply(data_clean,1,sum)
data$times_discover_in_all<-data_clean$times_discover_in_all
library("openxlsx")
library("Hmisc")
library("tidyverse")
data_ARGsub<-read.xlsx("C:/Users/USER/Desktop/lab/實驗/Metagenomic in DWDS/DATA/newDATA/ARG/ARGoap_out.xlsx",sheet=1,rowNames=F,sep.names=" ")
data_taxa<-read.xlsx("C:/Users/USER/Desktop/lab/實驗/Metagenomic in DWDS/DATA/newDATA/TAXA/rel abundance table/genus_rel_table.xlsx",sheet=1,rowNames=F,sep.names=" ")
#ARGdata名稱處理
data_ARGsub<-data_ARGsub%>%
separate(`ARGs abundance normalization aganist 16S`,into=c("type","subtype"),sep="__")
rownames(data_ARGsub)<-data_ARGsub$subtype
argclass<-data_ARGsub[,1:2]
data_ARGsub$type<-NULL
data_ARGsub$subtype<-NULL
data_ARGsub<-data_ARGsub[apply(data_ARGsub, 1, function(x) !all(x==0)),]
#此部分先篩選出在所有樣品中出現超過幾次之species，input型態row="species',col='sample'
data_clean<-data_ARGsub
data_clean[data_clean!=0]<-1
data_clean$times_discover_in_all<-apply(data_clean,1,sum)
data$times_discover_in_all<-data_clean$times_discover_in_all
View(data_ARGsub)
#此部分先篩選出在所有樣品中出現超過幾次之species，input型態row="species',col='sample'
data_clean<-data_ARGsub
data_clean[data_clean!=0]<-1
data_clean$times_discover_in_all<-apply(data_clean,1,sum)
dataARGsub$times_discover_in_all<-data_clean$times_discover_in_all
data_ARGsub$times_discover_in_all<-data_clean$times_discover_in_all
times_over7_genusdata<-filter(data,times_discover_in_all>=7)
library("tidyverse")
times_over7_genusdata<-filter(data_ARGsub,times_discover_in_all>=7)
times_over7_genusdata$times_discover_in_all<-NULL
transform_data<-as.data.frame(t(times_over7_genusdata))
View(times_over7_genusdata)
View(times_over7_genusdata)
transform_data<-as.data.frame(t(times_over7_genusdata))
#以下部分開始進行相關性分析
data<-transform_data
#rcorr()他的input要是matrix
data.matrix<-as.matrix(data)
corr<-rcorr(data.matrix,type= 'spearman')
#P值修正
corr_P_adj <- p.adjust(corr$P, method = 'BH')
matrix_corr_P_adj <- matrix(corr_P_adj,nrow=(length(corr$P)**0.5))
colnames(matrix_corr_P_adj)<-colnames(data)
rownames(matrix_corr_P_adj) <- colnames(data)
matrix_corr_P_adj[matrix_corr_P_adj>= 0.05] <- -1
matrix_corr_P_adj[matrix_corr_P_adj < 0.05 & matrix_corr_P_adj >= 0] <- 1
matrix_corr_P_adj[matrix_corr_P_adj == -1] <- 0
#先輸出一次有顯著相關但相關性未必足夠的矩陣
corr_significiant<-corr$r * matrix_corr_P_adj
#write.xlsx之output要是dataframe
corr_significiant.dataframe<-as.data.frame(corr_significiant)
write.xlsx(corr_significiant.dataframe,file="c:/Users/USER/Desktop/corr_significiant_FAMILIES.xlsx",rowNames=T,colNames=T)
#將計算出來之相關性大於0.8且p小於0.05者留下
corr$r[corr$r < 0.8] <- 0
corr_final <-corr$r * matrix_corr_P_adj
#計算相關性只會有半邊的矩陣(上面是多餘的)，我們只會需要下三角矩陣，且不需要對角矩陣(都為1)
corr_final[!lower.tri(corr_final)] <- 0
#有些數據因為是0所以算不出相關性(na)，去除
corr_final[is.na(corr_final)]<-0
corr_final.dataframe<-as.data.frame(corr_final)
write.xlsx(corr_final.dataframe, 'C:/Users/USER/Desktop/FAMILES_R0.7P0.05.xlsx',rowNames=T,colNames=T)
library("PMCMRplus")
library("dplyr")
library("tidyverse")
library("openxlsx")
library("stringr")
library("car")
library("FSA")
library("RColorBrewer")
library("ggsignif")
library("mdthemes")
data<-read.xlsx(xlsxFile = "C:/Users/USER/Desktop/lab/實驗/Metagenomic in DWDS/DATA/newDATA/ARC_analysis/mycolicibacterium/Contig taxa conc.xlsx",sheet=1,rowNames=T,colNames=T)
View(data)
#目的:比對某參數在兩個季節採樣間是否有顯著差異(排除原水)
data<-data[apply(data, 1, function(x) !all(x==0)),]
data<-as.data.frame(t(data))
data$sum<-apply(data,1,sum)
View(data)
data$location<-c("Raw","Raw","Raw","Finished","Finished","Finished","Upstream","Upstream","Upstream","Midstream","Midstream","Midstream","Downstream","Downstream","Downstream")
data$location<-factor(data$location,levels = c("Raw","Finished","Upstream","Midstream","Downstream"))
View(data)
varaible_and_group<-g__mycobacterium~location#想測試的變數跟組別
varaible_and_group<-`g__mycobacterium`~location#想測試的變數跟組別
View(data)
varaible_and_group<-`g_Mycobacterium`~location#想測試的變數跟組別
#我們必須先檢查數據是不是常態分布及變異數的同質性，才能決定我們要用的檢定方法。
#檢查數據變異數的同質性，可以使用levenes test
#如果levenes test 的結果p>0.05，那我們可以認為以這幾個組別間的變異數沒有明顯差異，他們是同質的P
{homo<-leveneTest(varaible_and_group,data = data)
homo
if (homo$`Pr(>F)`[1]>0.05){
print("data is homo")
}else{print ("data is nonhomo")}
#接著檢查數據是否是常態分布的
res.aov <- aov(varaible_and_group, data = data)
plot(res.aov,2)#這個是QQplot 可以透過這張圖來看一下有哪些點可以拿掉。
aov_residuals <- residuals(object = res.aov )
#利用shapiro.test來檢驗數據是不是常態的，如果p>0.05那麼數據就是常態的
a<-shapiro.test(x = aov_residuals)
if(a["p.value"]>0.05){
print("data is normal distribution")
}else{print("data is  not a normal distribution")}
#如果沒問題(常態且同質)就可以看ANOVA的結果了
if (a["p.value"]>0.05 && homo$`Pr(>F)`[1]>0.05){
#如果p<0.05，那表是多組間的差距是不同的，那我們可以用事後檢定來看到底是誰不一樣
print ("we can use anova ")
anova_p<-summary(res.aov)
if (anova_p[[1]]$`Pr(>F)`[1]<0.05){
print ("anova p<0.05, use TukeyHSD")
TukeyHSD(res.aov)
}else{
print("anova p>0.05,difference is insignificiant")
}
}else{
#數據在anova的兩項假設中有一項不符合，因此我們要使用kruskal-wallis來檢定
print ("use kruskal wallise rank sum test")
kruskal_output<-kruskal.test(varaible_and_group, data = data)
if (kruskal_output$p.value< 0.05){
#kruskal-wallis 檢定 p <0.05，表示組間有差距，因此我們要使用事後檢定
#我們可以使用Dunntest來看看是哪一組不同
PT = dunnTest(varaible_and_group, data = data,
method="bh")    # Can adjust p-values;
print (PT)
}else{
print ("kruskal-wallis p>0.05,difference is insignificiant")
}
}
}
varaible_and_group<-`g__Mycobacterium`~location#想測試的變數跟組別
#我們必須先檢查數據是不是常態分布及變異數的同質性，才能決定我們要用的檢定方法。
#檢查數據變異數的同質性，可以使用levenes test
#如果levenes test 的結果p>0.05，那我們可以認為以這幾個組別間的變異數沒有明顯差異，他們是同質的P
{homo<-leveneTest(varaible_and_group,data = data)
homo
if (homo$`Pr(>F)`[1]>0.05){
print("data is homo")
}else{print ("data is nonhomo")}
#接著檢查數據是否是常態分布的
res.aov <- aov(varaible_and_group, data = data)
plot(res.aov,2)#這個是QQplot 可以透過這張圖來看一下有哪些點可以拿掉。
aov_residuals <- residuals(object = res.aov )
#利用shapiro.test來檢驗數據是不是常態的，如果p>0.05那麼數據就是常態的
a<-shapiro.test(x = aov_residuals)
if(a["p.value"]>0.05){
print("data is normal distribution")
}else{print("data is  not a normal distribution")}
#如果沒問題(常態且同質)就可以看ANOVA的結果了
if (a["p.value"]>0.05 && homo$`Pr(>F)`[1]>0.05){
#如果p<0.05，那表是多組間的差距是不同的，那我們可以用事後檢定來看到底是誰不一樣
print ("we can use anova ")
anova_p<-summary(res.aov)
if (anova_p[[1]]$`Pr(>F)`[1]<0.05){
print ("anova p<0.05, use TukeyHSD")
TukeyHSD(res.aov)
}else{
print("anova p>0.05,difference is insignificiant")
}
}else{
#數據在anova的兩項假設中有一項不符合，因此我們要使用kruskal-wallis來檢定
print ("use kruskal wallise rank sum test")
kruskal_output<-kruskal.test(varaible_and_group, data = data)
if (kruskal_output$p.value< 0.05){
#kruskal-wallis 檢定 p <0.05，表示組間有差距，因此我們要使用事後檢定
#我們可以使用Dunntest來看看是哪一組不同
PT = dunnTest(varaible_and_group, data = data,
method="bh")    # Can adjust p-values;
print (PT)
}else{
print ("kruskal-wallis p>0.05,difference is insignificiant")
}
}
}
varaible_and_group<-`g__Mycolicibacterium`~location#想測試的變數跟組別
#我們必須先檢查數據是不是常態分布及變異數的同質性，才能決定我們要用的檢定方法。
#檢查數據變異數的同質性，可以使用levenes test
#如果levenes test 的結果p>0.05，那我們可以認為以這幾個組別間的變異數沒有明顯差異，他們是同質的P
{homo<-leveneTest(varaible_and_group,data = data)
homo
if (homo$`Pr(>F)`[1]>0.05){
print("data is homo")
}else{print ("data is nonhomo")}
#接著檢查數據是否是常態分布的
res.aov <- aov(varaible_and_group, data = data)
plot(res.aov,2)#這個是QQplot 可以透過這張圖來看一下有哪些點可以拿掉。
aov_residuals <- residuals(object = res.aov )
#利用shapiro.test來檢驗數據是不是常態的，如果p>0.05那麼數據就是常態的
a<-shapiro.test(x = aov_residuals)
if(a["p.value"]>0.05){
print("data is normal distribution")
}else{print("data is  not a normal distribution")}
#如果沒問題(常態且同質)就可以看ANOVA的結果了
if (a["p.value"]>0.05 && homo$`Pr(>F)`[1]>0.05){
#如果p<0.05，那表是多組間的差距是不同的，那我們可以用事後檢定來看到底是誰不一樣
print ("we can use anova ")
anova_p<-summary(res.aov)
if (anova_p[[1]]$`Pr(>F)`[1]<0.05){
print ("anova p<0.05, use TukeyHSD")
TukeyHSD(res.aov)
}else{
print("anova p>0.05,difference is insignificiant")
}
}else{
#數據在anova的兩項假設中有一項不符合，因此我們要使用kruskal-wallis來檢定
print ("use kruskal wallise rank sum test")
kruskal_output<-kruskal.test(varaible_and_group, data = data)
if (kruskal_output$p.value< 0.05){
#kruskal-wallis 檢定 p <0.05，表示組間有差距，因此我們要使用事後檢定
#我們可以使用Dunntest來看看是哪一組不同
PT = dunnTest(varaible_and_group, data = data,
method="bh")    # Can adjust p-values;
print (PT)
}else{
print ("kruskal-wallis p>0.05,difference is insignificiant")
}
}
}
library(vegan)
library(ecodist)
library(ggplot2)
library(openxlsx)
library(RColorBrewer)
dbpata<-read.xlsx("C:/Users/USER/Desktop/lab/實驗/Metagenomic in DWDS/DATA/newDATA/TAXA/rel abundance table/species_rel_table.xlsx",sheet=1,rowNames=F,colNames=T,sep.names=" ")
groupata<-read.xlsx("C:/Users/USER/Desktop/lab/實驗/Metagenomic in DWDS/DATA/newDATA/TAXA/groupdata.xlsx",sheet=1,rowNames=T,colNames=T,sep.names=" ")
arg_data<-read.xlsx("C:/Users/USER/Desktop/lab/實驗/Metagenomic in DWDS/DATA/newDATA/ARG/ARGoap_out.xlsx",sheet=1,rowNames=T,colNames=T,sep.names=" ")
#這個不一定要，下面這個只是把全部都是0的rows清掉
#dbpata[dbpata<0.0001]<-0
#dbpata<-dbpata[apply(dbpata, 1, function(x) !all(x==0)),]
#dbpata1<-as.data.frame(apply(dbpata,2,function(x) x/sum(x)))
rownames(dbpata)<-dbpata$Species
library(vegan)
function (..., list = character(), package = NULL, lib.loc = NULL,
verbose = getOption("verbose"), envir = .GlobalEnv, overwrite = TRUE)
{
fileExt <- function(x) {
db <- grepl("\\.[^.]+\\.(gz|bz2|xz)$", x)
ans <- sub(".*\\.", "", x)
ans[db] <- sub(".*\\.([^.]+\\.)(gz|bz2|xz)$", "\\1\\2",
x[db])
ans
}
my_read_table <- function(...) {
lcc <- Sys.getlocale("LC_COLLATE")
on.exit(Sys.setlocale("LC_COLLATE", lcc))
Sys.setlocale("LC_COLLATE", "C")
read.table(...)
}
stopifnot(is.character(list))
names <- c(as.character(substitute(list(...))[-1L]), list)
if (!is.null(package)) {
if (!is.character(package))
stop("'package' must be a character vector or NULL")
}
paths <- find.package(package, lib.loc, verbose = verbose)
if (is.null(lib.loc))
paths <- c(path.package(package, TRUE), if (!length(package)) getwd(),
paths)
paths <- unique(normalizePath(paths[file.exists(paths)]))
paths <- paths[dir.exists(file.path(paths, "data"))]
dataExts <- tools:::.make_file_exts("data")
if (length(names) == 0L) {
db <- matrix(character(), nrow = 0L, ncol = 4L)
for (path in paths) {
entries <- NULL
packageName <- if (file_test("-f", file.path(path,
"DESCRIPTION")))
basename(path)
else "."
if (file_test("-f", INDEX <- file.path(path, "Meta",
"data.rds"))) {
entries <- readRDS(INDEX)
}
else {
dataDir <- file.path(path, "data")
entries <- tools::list_files_with_type(dataDir,
"data")
if (length(entries)) {
entries <- unique(tools::file_path_sans_ext(basename(entries)))
entries <- cbind(entries, "")
}
}
if (NROW(entries)) {
if (is.matrix(entries) && ncol(entries) == 2L)
db <- rbind(db, cbind(packageName, dirname(path),
entries))
else warning(gettextf("data index for package %s is invalid and will be ignored",
sQuote(packageName)), domain = NA, call. = FALSE)
}
}
colnames(db) <- c("Package", "LibPath", "Item", "Title")
footer <- if (missing(package))
paste0("Use ", sQuote(paste("data(package =", ".packages(all.available = TRUE))")),
"\n", "to list the data sets in all *available* packages.")
else NULL
y <- list(title = "Data sets", header = NULL, results = db,
footer = footer)
class(y) <- "packageIQR"
return(y)
}
paths <- file.path(paths, "data")
for (name in names) {
found <- FALSE
for (p in paths) {
tmp_env <- if (overwrite)
envir
else new.env()
if (file_test("-f", file.path(p, "Rdata.rds"))) {
rds <- readRDS(file.path(p, "Rdata.rds"))
if (name %in% names(rds)) {
found <- TRUE
if (verbose)
message(sprintf("name=%s:\t found in Rdata.rds",
name), domain = NA)
thispkg <- sub(".*/([^/]*)/data$", "\\1",
p)
thispkg <- sub("_.*$", "", thispkg)
thispkg <- paste0("package:", thispkg)
objs <- rds[[name]]
lazyLoad(file.path(p, "Rdata"), envir = tmp_env,
filter = function(x) x %in% objs)
break
}
else if (verbose)
message(sprintf("name=%s:\t NOT found in names() of Rdata.rds, i.e.,\n\t%s\n",
name, paste(names(rds), collapse = ",")),
domain = NA)
}
if (file_test("-f", file.path(p, "Rdata.zip"))) {
warning("zipped data found for package ", sQuote(basename(dirname(p))),
".\nThat is defunct, so please re-install the package.",
domain = NA)
if (file_test("-f", fp <- file.path(p, "filelist")))
files <- file.path(p, scan(fp, what = "",
quiet = TRUE))
else {
warning(gettextf("file 'filelist' is missing for directory %s",
sQuote(p)), domain = NA)
next
}
}
else {
files <- list.files(p, full.names = TRUE)
}
files <- files[grep(name, files, fixed = TRUE)]
if (length(files) > 1L) {
o <- match(fileExt(files), dataExts, nomatch = 100L)
paths0 <- dirname(files)
paths0 <- factor(paths0, levels = unique(paths0))
files <- files[order(paths0, o)]
}
if (length(files)) {
for (file in files) {
if (verbose)
message("name=", name, ":\t file= ...",
.Platform$file.sep, basename(file), "::\t",
appendLF = FALSE, domain = NA)
ext <- fileExt(file)
if (basename(file) != paste0(name, ".", ext))
found <- FALSE
else {
found <- TRUE
zfile <- file
zipname <- file.path(dirname(file), "Rdata.zip")
if (file.exists(zipname)) {
Rdatadir <- tempfile("Rdata")
dir.create(Rdatadir, showWarnings = FALSE)
topic <- basename(file)
rc <- .External(C_unzip, zipname, topic,
Rdatadir, FALSE, TRUE, FALSE, FALSE)
if (rc == 0L)
zfile <- file.path(Rdatadir, topic)
}
if (zfile != file)
on.exit(unlink(zfile))
switch(ext, R = , r = {
library("utils")
sys.source(zfile, chdir = TRUE, envir = tmp_env)
}, RData = , rdata = , rda = load(zfile,
envir = tmp_env), TXT = , txt = , tab = ,
tab.gz = , tab.bz2 = , tab.xz = , txt.gz = ,
txt.bz2 = , txt.xz = assign(name, my_read_table(zfile,
header = TRUE, as.is = FALSE), envir = tmp_env),
CSV = , csv = , csv.gz = , csv.bz2 = ,
csv.xz = assign(name, my_read_table(zfile,
header = TRUE, sep = ";", as.is = FALSE),
envir = tmp_env), found <- FALSE)
}
if (found)
break
}
if (verbose)
message(if (!found)
"*NOT* ", "found", domain = NA)
}
if (found)
break
}
if (!found) {
warning(gettextf("data set %s not found", sQuote(name)),
domain = NA)
}
else if (!overwrite) {
for (o in ls(envir = tmp_env, all.names = TRUE)) {
if (exists(o, envir = envir, inherits = FALSE))
warning(gettextf("an object named %s already exists and will not be overwritten",
sQuote(o)))
else assign(o, get(o, envir = tmp_env, inherits = FALSE),
envir = envir)
}
rm(tmp_env)
}
}
invisible(names)
}
library(vegan)
library(ecodist)
library(ggplot2)
library(openxlsx)
library(RColorBrewer)
dbpata<-read.xlsx("C:/Users/USER/Desktop/lab/實驗/Metagenomic in DWDS/DATA/newDATA/TAXA/rel abundance table/species_rel_table.xlsx",sheet=1,rowNames=F,colNames=T,sep.names=" ")
groupata<-read.xlsx("C:/Users/USER/Desktop/lab/實驗/Metagenomic in DWDS/DATA/newDATA/TAXA/groupdata.xlsx",sheet=1,rowNames=T,colNames=T,sep.names=" ")
arg_data<-read.xlsx("C:/Users/USER/Desktop/lab/實驗/Metagenomic in DWDS/DATA/newDATA/ARG/ARGoap_out.xlsx",sheet=1,rowNames=T,colNames=T,sep.names=" ")
#這個不一定要，下面這個只是把全部都是0的rows清掉
#dbpata[dbpata<0.0001]<-0
#dbpata<-dbpata[apply(dbpata, 1, function(x) !all(x==0)),]
#dbpata1<-as.data.frame(apply(dbpata,2,function(x) x/sum(x)))
rownames(dbpata)<-dbpata$Species
dbpata<-dbpata[,-(1:7)]
dbpata <-as.data.frame(t(dbpata))
arg_data<-as.data.frame(t(arg_data))
View(arg_data)
View(dbpata)
dbpata <- decostand(dbpata, method = 'hellinger')
dbpata<-dbpata[-c(1:6),]
groupata<-groupata[-c(1:6),]
arg_data<-arg_data[-c(1:6),]
taxa_bray<-vegdist(dbpata, method="bray")
arg_bray<-vegdist(arg_data, method="bray")
#普氏分析可以用pcoa,pca,nmds等都可以，看哪個分離的好
pcoa1 = cmdscale(taxa_bray, eig=TRUE)
pcoa2 = cmdscale(arg_bray, eig=TRUE)
#mds.taxa<-monoMDS(taxa_bray)
#mds.arg<-monoMDS(arg_bray)
pro.g.s<-procrustes(pcoa1,pcoa2,symmetric = T)
summary(pro.g.s)
protest(pcoa1,pcoa2)
plot(pro.g.s, kind = 1,type="text")
Y<-cbind(data.frame(pro.g.s$Yrot),data.frame(pro.g.s$X))
X<-data.frame(pro.g.s$rotation)
Y$sample<-rownames(Y)
Y <- merge(Y, groupata, by = 'sample')
Y$location<-factor(Y$location,levels = c("Raw","Finished","Upstream","Midstream","Downstream"))
#color
RColorBrewer::display.brewer.all()
display.brewer.pal(n=12,name="Set3")
brewer.pal(n=12,name="Set3")
color<-c("#FB8072","#BEBADA","#80B1D3","#FDB462","#B3DE69")
#if you use the Rcolor brewer,你需要利用scale_color_brewer(palette="paletee_name")
#自選顏色的話用Scale_color_manual(values=color_vector)
#ggplot
p <- ggplot(Y) +
geom_segment(aes(x = X1, y = X2,
xend = Dim1, yend = Dim2,color=location),
# geom_segment 绘制两点间的直线
size = 0.75,linetype="dashed",alpha=0.7) +
geom_point(aes(X1, X2, color =location),shape=16,size = 3,alpha=0.5) +
geom_point(aes(Dim1,Dim2,color = location),shape=17,size = 3,alpha=0.5) +
scale_color_manual("Location",values = color) +theme_bw() +labs( title="Procrustes analysis")+
labs(x = 'Dimension 1', y = 'Dimension 2', color = '') +
geom_vline(xintercept = 0, color = 'gray', linetype = 2, size = 0.3) +
geom_hline(yintercept = 0, color = 'gray', linetype = 2, size = 0.3) +
annotate('text', label = sprintf('M^2 == 0.0282 '),
x = 0.2, y = 0.29, size =4, parse = TRUE) +
annotate('text', label = 'P==0.001',
x = 0.2, y = 0.275, size = 4, parse = TRUE)+theme(axis.title = element_text(size=12),legend.title= element_text(size=12),legend.text = element_text(size=12))
p
